{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9354c93f",
   "metadata": {},
   "source": [
    "# US Accidents Dataset - Initial Loading and Exploration\n",
    "\n",
    "**Objective**: Load the US Accidents dataset and perform initial exploration to understand the data structure, quality, and key characteristics.\n",
    "\n",
    "**Dataset**: US Accidents (2016-2023)\n",
    "- Source: Kaggle (sobhanmoosavi/us-accidents)\n",
    "- Size: 2.9 GB, 7.7M+ records, 46 columns\n",
    "- Geographic Coverage: 49 US states\n",
    "\n",
    "## Notebook Contents\n",
    "1. Environment Setup\n",
    "2. Data Loading\n",
    "3. Basic Statistics\n",
    "4. Data Quality Assessment\n",
    "5. Geographic Coverage\n",
    "6. Temporal Patterns\n",
    "7. Severity Distribution\n",
    "8. Initial Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54fd332",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from enum import IntEnum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "print(\"‚úì Required packages imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from config import Config\n",
    "\n",
    "# Setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c4744e",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Let's load the dataset. We initially define optimized datatypes for improved storage & processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ed168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show file sizes\n",
    "fars_csv_names = [\"accident\", \"drugs\", \"person\"]\n",
    "class FARS_DATA_INDEX(IntEnum):\n",
    "    ACCIDENT = 0\n",
    "    DRUGS = 1\n",
    "    PERSON = 2\n",
    "years = [2018, 2019, 2023]\n",
    "\n",
    "for year in years:\n",
    "    for filename in fars_csv_names:\n",
    "        data_path = Config.FARS_RAW_DIR / f\"{year}/{filename}.csv\"\n",
    "        print(f\"{year} {filename} file size: {data_path.stat().st_size / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimized data types\n",
    "dtype_dict = {\n",
    "    'ST_CASE': 'int32',\n",
    "    'STATENAME': 'category',\n",
    "    'PEDS': 'int16',\n",
    "    'PERNOTMVIT': 'int16',\n",
    "    'VE_TOTAL': 'int16',\n",
    "    'VE_FORMS': 'int16',\n",
    "    'PVH_INVL': 'int16',\n",
    "    'PERSONS': 'int16',\n",
    "    'PERMVIT': 'int16',\n",
    "    'COUNTYNAME': 'string',\n",
    "    'MONTH': 'int8',\n",
    "    'DAY': 'int8',\n",
    "    'DAY_WEEKNAME': 'category',\n",
    "    'YEAR': 'int16',\n",
    "    'HOUR': 'int8',\n",
    "    'MINUTE': 'int8',\n",
    "    'TWAY_ID': 'string',\n",
    "    'ROUTENAME': 'string',\n",
    "    'RUR_URBNAME': 'category',\n",
    "    'FUNC_SYSNAME': 'string',#'category',\n",
    "    'RD_OWNERNAME': 'category',\n",
    "    'NHSNAME': 'category',\n",
    "    'SP_JURNAME': 'category',\n",
    "    'HARM_EVNAME': 'category',\n",
    "    'MAN_COLLNAME': 'string',#'category',\n",
    "    'RELJCT2NAME': 'category',\n",
    "    'TYP_INTNAME': 'category',\n",
    "    'REL_ROADNAME': 'category',\n",
    "    'WRK_ZONE': 'category',\n",
    "    'LGT_CONDNAME': 'category',\n",
    "    'WEATHERNAME': 'category',\n",
    "    'SCH_BUS': 'boolean',\n",
    "    'RAIL': 'string',\n",
    "    'NOT_HOUR': 'int8',\n",
    "    'NOT_MIN': 'int8',\n",
    "    'ARR_HOUR': 'int8',\n",
    "    'ARR_MIN': 'int8',\n",
    "    'HOSP_HR': 'int8',\n",
    "    'HOSP_MN': 'int8',\n",
    "    'DRUGRESNAME': 'string',#'category',\n",
    "    'AGE': 'int8',\n",
    "    'PER_TYPNAME': 'category',\n",
    "    'INJ_SEV': 'int8',\n",
    "    'ALC_RES': 'int16',\n",
    "}\n",
    "\n",
    "print(\"Data type optimization configured\")\n",
    "print(f\"Categorical columns: {sum(1 for v in dtype_dict.values() if v == 'category')}\")\n",
    "print(f\"Boolean columns: {sum(1 for v in dtype_dict.values() if v == 'bool')}\")\n",
    "print(f\"Float32 columns: {sum(1 for v in dtype_dict.values() if v == 'float32')}\")\n",
    "\n",
    "# Load the raw data\n",
    "fars_data = [[pd.DataFrame() for _ in range(len(fars_csv_names))] for _ in range(len(years))]\n",
    "for i, year in enumerate(years):\n",
    "    for j, filename in enumerate(fars_csv_names):\n",
    "        data_path = Config.FARS_RAW_DIR / f\"{year}/{filename}.csv\"\n",
    "        fars_data[i][j] = pd.read_csv(data_path, dtype=dtype_dict, sep=',', encoding='latin_1')\n",
    "        print(f\"\\n‚úì Dataset loaded: {data_path} {len(fars_data[i][j]):,} rows √ó {len(fars_data[i][j].columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns of interest/drop unimportant columns\n",
    "# This will probably be replaced by something determined by PCA\n",
    "fars_accident_cols = [\n",
    "    'ST_CASE', 'STATENAME', 'PEDS', 'PERNOTMVIT', 'VE_TOTAL', 'VE_FORMS', 'PVH_INVL', 'PERSONS', 'PERMVIT',\n",
    "    'COUNTYNAME', 'MONTH', 'DAY', 'DAY_WEEKNAME', 'YEAR', 'HOUR', 'MINUTE', 'TWAY_ID', 'ROUTENAME',\n",
    "    'RUR_URBNAME', 'FUNC_SYSNAME', 'RD_OWNERNAME', 'NHSNAME', 'SP_JURNAME', 'HARM_EVNAME',\n",
    "    'MAN_COLLNAME', 'RELJCT2NAME', 'TYP_INTNAME', 'REL_ROADNAME', 'WRK_ZONE', 'LGT_CONDNAME', 'WEATHERNAME',\n",
    "    'SCH_BUSNAME', 'RAIL', 'NOT_HOUR', 'NOT_MIN', 'ARR_HOUR', 'ARR_MIN', 'HOSP_HR', 'HOSP_MN'\n",
    "    ]\n",
    "fars_drugs_cols = ['ST_CASE', 'PER_NO', 'DRUGRESNAME']\n",
    "fars_person_cols = ['ST_CASE', 'PER_NO', 'AGE', 'PER_TYP', 'INJ_SEV', 'ALC_RES']\n",
    "fars_col_subsets = [fars_accident_cols, fars_drugs_cols, fars_person_cols]\n",
    "for i, year in enumerate(years):\n",
    "    for j, fars_col_subset in enumerate(fars_col_subsets):\n",
    "        print(f\"Dropping unwanted columns from {fars_csv_names[j]}:\")\n",
    "        fars_data[i][j] = fars_data[i][j][fars_col_subset]\n",
    "        print(f\"\\n‚úì Columns dropped, new dataset shape is: {len(fars_data[i][j]):,} rows √ó {len(fars_data[i][j].columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd38a5",
   "metadata": {},
   "source": [
    "## 3. Joining FARS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba3e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "# TODO personrf has indications of whether or not a drug/alcohol test was refused\n",
    "missing = pd.DataFrame({\n",
    "        'Column': [],\n",
    "        'Missing_Count': [],\n",
    "        'Missing_Percentage': []\n",
    "    })\n",
    "\n",
    "remap_vals = {\n",
    "    'ROUTENAME': {\n",
    "        'County': 'County Road',\n",
    "        'Township': 'Local Street - Township',\n",
    "        'Municipal': 'Local Street - Municipality',\n",
    "        'Local Street - Frontage Road': 'Other',\n",
    "        'Parkway Marker or Forest Route Marker [Specify:]': 'Other',\n",
    "        'Off-Interstate Business Marker': 'Other',\n",
    "        'Secondary Route': 'Other',\n",
    "        'Bureau of Indian Affairs': 'Other'\n",
    "        },\n",
    "    'FUNC_SYSNAME': {\n",
    "        'Other Freeways and Expressways': 'Principal Arterial - Other Freeways and Expressways',\n",
    "        'Other Principal Arterial': 'Principal Arterial - Other'\n",
    "        },\n",
    "    'MAN_COLLNAME': {\n",
    "        'First Harmful Event was Not a Collision with Motor Vehicle In-Transport': 'Not a Collision with Motor Vehicle In-Transport'\n",
    "        },\n",
    "    'DRUGRESNAME': {\n",
    "        'Narcotic Analgesics': 'Narcotics',\n",
    "        'Phencyclidine (PCP)': 'Other Drug (Specify:)',\n",
    "        'Dissociative Anesthetics': 'Other Drug (Specify:)',\n",
    "        'Non-Psychoactive/Other Drugs': 'None Detected/Below Threshold',\n",
    "    }\n",
    "}\n",
    "\n",
    "missing_vals = {\n",
    "    'HOUR': [99],\n",
    "    'MINUTE': [99],\n",
    "    'TWAY_ID': [999999999999999999999999999999],\n",
    "    'ROUTENAME': ['Unknown/Not Reported', 'Trafficway Not in State Inventory', 'Unknown'], #Changes 2023\n",
    "    'RUR_URBNAME': [6, 8, 9],\n",
    "    'FUNC_SYSNAME': [96, 98, 99], #Changes 2023\n",
    "    'RD_OWNERNAME': [96, 98, 99],\n",
    "    'NHSNAME': [9],\n",
    "    'SP_JURNAME': [9],\n",
    "    'HARM_EVNAME': ['Reported as Unknown'],\n",
    "    'MAN_COLLNAME': ['Unknown'], #Changes 2019\n",
    "    'RELJCT2NAME': ['Reported as Unknown', 'Not Reported'],\n",
    "    'TYP_INTNAME': ['Reported as Unknown', 'Not Reported'], #Change 2020\n",
    "    'REL_ROADNAME': ['Reported as Unknown', 'Not Reported'],\n",
    "    'LGT_CONDNAME': ['Other', 'Not Reported', 'Reported as Unknown'],\n",
    "    'WEATHERNAME': ['Not Reported', 'Unknown', 'Reported as Unknown'],\n",
    "    'RAIL': ['9999999'],\n",
    "    'NOT_HOUR': [88, 99],\n",
    "    'NOT_MIN': [88, 98, 99],\n",
    "    'DRUGRESNAME': ['Reported as Unknown if Tested for Drugs', 'Tested for Drugs, Results Unknown', 'Not Reported'], #Changes 2022\n",
    "    'ALC_RES': [999], #Needs more than just dealing with missing values\n",
    "    'INJ_SEV': [9],\n",
    "    'AGE': [99] #97 or older are all recorded as 97\n",
    "}\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    for j, dataset in enumerate(fars_data[i]):\n",
    "        for column in dataset:\n",
    "            if column in remap_vals:\n",
    "                for value in remap_vals[column]:\n",
    "                    dataset.loc[dataset[column] == value, column] = remap_vals[column][value]\n",
    "\n",
    "            if column in missing_vals:\n",
    "                for value in missing_vals[column]:\n",
    "                    dataset.loc[dataset[column] == value, column] = None\n",
    "\n",
    "print(\"Remapped non-missing values & replaced missing values\")\n",
    "\n",
    "for i, dataset in enumerate(fars_data[0]):\n",
    "    for column in dataset:\n",
    "        missing_count = sum([fars_data[j][i][column].isna().sum() for j in range(len(fars_data))])\n",
    "        total_count = sum([len(fars_data[j][i][column]) for j in range(len(fars_data))])\n",
    "        new_missing_frame = pd.DataFrame({\n",
    "            'Column': [f\"{fars_csv_names[i]}.{column}\"],\n",
    "            'Missing_Count': [missing_count],\n",
    "            'Missing_Percentage': [((missing_count / total_count) * 100).round(2)]\n",
    "        })\n",
    "        missing = pd.concat([missing, new_missing_frame])\n",
    "\n",
    "missing = missing[missing['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "print(f\"Columns with missing values: {len(missing)} out of {sum([len(dataset.columns) for dataset in fars_data[0]])}\")\n",
    "print(\"\\nColumns with missing data:\")\n",
    "missing = missing.reset_index(drop=True)\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "missing_top = missing.head(10)\n",
    "ax.barh(missing_top['Column'], missing_top['Missing_Percentage'], color='coral')\n",
    "ax.set_xlabel('Missing Percentage (%)', fontsize=12)\n",
    "ax.set_title('Top 10 Columns with Missing Data', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec407629",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['TOTAL_HARM', 'INTOXICATED_DRIVER_INVOLVED']\n",
    "columns = fars_accident_cols + new_cols\n",
    "\n",
    "joined_years_data = []\n",
    "\n",
    "for i, datasets in enumerate(fars_data):\n",
    "    joined_year_data = datasets[FARS_DATA_INDEX.ACCIDENT].copy()\n",
    "    for col in new_cols:\n",
    "        joined_year_data[col] = pd.NA\n",
    "\n",
    "    for j, accident in datasets[FARS_DATA_INDEX.ACCIDENT].iterrows():\n",
    "        num_persons = accident['PERSONS']\n",
    "        person_row_number = datasets[FARS_DATA_INDEX.PERSON].index[datasets[FARS_DATA_INDEX.PERSON]['ST_CASE'] == accident['ST_CASE']][0]\n",
    "        drugs_row_number = datasets[FARS_DATA_INDEX.DRUGS]['ST_CASE'].index[datasets[FARS_DATA_INDEX.DRUGS]['ST_CASE'] == accident['ST_CASE']][0]\n",
    "        driver_person_numbers = []\n",
    "        intoxicated_driver_involved = False\n",
    "        for column in new_cols:\n",
    "            if column == new_cols[0]: #Add a sum for all injuries\n",
    "                total_harm = 0\n",
    "                for k in range(num_persons):\n",
    "                    person_harm = datasets[FARS_DATA_INDEX.PERSON].loc[person_row_number + k, 'INJ_SEV']\n",
    "                    if person_harm <= 4:\n",
    "                        total_harm += person_harm\n",
    "                    elif person_harm == 5: #Injured, but unknown severity\n",
    "                        total_harm += 1\n",
    "                joined_year_data.loc[j, column] = total_harm\n",
    "\n",
    "            elif column == new_cols[1]: #Check if any intoxicated drivers were involved\n",
    "                drunk_drivers = 0\n",
    "                for k in range(num_persons):\n",
    "                    bac = datasets[FARS_DATA_INDEX.PERSON].loc[person_row_number + k, 'ALC_RES']\n",
    "                    per_type = datasets[FARS_DATA_INDEX.PERSON].loc[person_row_number + k, 'PER_TYP']\n",
    "                    driver_person_number = datasets[FARS_DATA_INDEX.PERSON].loc[person_row_number + k, 'PER_NO']\n",
    "                    if bac > 80 and bac not in list(range(995, 999)) and per_type == 1: #0.080% is the legal limit in all states, some states have additional punishments for certain higher values. Special values are assumed to be none\n",
    "                        intoxicated_driver_involved = True\n",
    "                        break\n",
    "\n",
    "                    drugs_idx = drugs_row_number\n",
    "                    drugs_row = datasets[FARS_DATA_INDEX.DRUGS].iloc[drugs_idx]\n",
    "                    while drugs_row['ST_CASE'] == accident['ST_CASE'] and drugs_row['PER_NO'] == driver_person_number:\n",
    "                        result = drugs_row['DRUGRESNAME']\n",
    "                        if pd.notna(result) and (not (result in [None, 'Test Not Given', 'None Detected/Below Threshold'])):\n",
    "                            intoxicated_driver_involved = True\n",
    "                            break\n",
    "                        if drugs_idx == len(datasets[FARS_DATA_INDEX.DRUGS]) - 1:\n",
    "                            break\n",
    "                        drugs_idx += 1\n",
    "                        drugs_row = datasets[FARS_DATA_INDEX.DRUGS].iloc[drugs_idx]\n",
    "                    \n",
    "                    if intoxicated_driver_involved:\n",
    "                        break\n",
    "\n",
    "            joined_year_data.loc[j, 'INTOXICATED_DRIVER_INVOLVED'] = intoxicated_driver_involved\n",
    "    joined_years_data += [joined_year_data]\n",
    "joined_data = pd.concat(joined_years_data)\n",
    "\n",
    "display(joined_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5c534",
   "metadata": {},
   "source": [
    "## 4. Geographic Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dcd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State distribution\n",
    "state_counts = joined_data['STATENAME'].value_counts().head(15)\n",
    "print(\"Top 15 States by fatal accident Count:\")\n",
    "print(\"=\" * 40)\n",
    "for state, count in state_counts.items():\n",
    "    pct = (count / len(joined_data)) * 100\n",
    "    print(f\"{state:10s} {count:>10,}  ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize state distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "state_counts.plot(kind='bar', ax=ax, color='steelblue')\n",
    "ax.set_title('Top 15 States by Number of Fatal Accidents', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('State', fontsize=12)\n",
    "ax.set_ylabel('Number of Fatal Accidents', fontsize=12)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{float(x/1000)}K'))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6f5c8",
   "metadata": {},
   "source": [
    "## 5. Temporal Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e117544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidents by hour of day\n",
    "hour_counts = joined_data['HOUR'].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(hour_counts.index, hour_counts.values, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "ax.fill_between(hour_counts.index, hour_counts.values, alpha=0.3)\n",
    "ax.set_title('Fatal Accident Distribution by Hour of Day', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "ax.set_ylabel('Number of Accidents', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ca6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidents by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_counts = joined_data['DAY_WEEKNAME'].value_counts().reindex(day_order)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "day_counts.plot(kind='bar', ax=ax, color='coral')\n",
    "ax.set_title('Fatal Accident Distribution by Day of Week', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Day of Week', fontsize=12)\n",
    "ax.set_ylabel('Number of Fatal Accidents', fontsize=12)\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{int(x/1000)}K'))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc911a4f",
   "metadata": {},
   "source": [
    "**Observation:** The weekend has a higher number of fatal accidents compared to working days, let's dive into the hourly distribution on weekends to see why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaeab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidents by hour of day on weekends\n",
    "\n",
    "hour_counts = joined_data[joined_data['DAY_WEEKNAME'].isin(['Saturday', 'Sunday'])]['HOUR'].value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(hour_counts.index, hour_counts.values, marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "ax.fill_between(hour_counts.index, hour_counts.values, alpha=0.3)\n",
    "ax.set_title('Accident Distribution by Hour of Day (Weekends)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Hour of Day', fontsize=12)\n",
    "ax.set_ylabel('Number of Accidents', fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xticks(range(0, 24))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the peak hours for accidents\n",
    "\n",
    "min_accidents = hour_counts.min()\n",
    "peak_hours = hour_counts.sort_values(ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Peak Hours for Accidents:\\n\")\n",
    "for hour, count in peak_hours.items():\n",
    "    print(f\"At {int(hour):02d}h: {count:,} accidents ({count/min_accidents:.2f}x minimum)\")  \n",
    "\n",
    "print(f\"\\nMinimum Accidents in a Single Hour (weekends): {min_accidents:,} accidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea277fc4",
   "metadata": {},
   "source": [
    "**Observation:** The rush hour patterns are less pronounced on weekends compared to weekdays. While there are still peaks in accident counts during late morning and early afternoon hours, the overall distribution is more uniform throughout the day. This suggests that weekend traffic is less influenced by traditional work commute times, leading to a more even spread of accidents across different hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbba4cb",
   "metadata": {},
   "source": [
    "## 6. Severity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Severity distribution\n",
    "severity_counts = joined_data['TOTAL_HARM'].value_counts().sort_index()\n",
    "\n",
    "print(\"Accident Severity Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Level | Count      | Percentage\")\n",
    "print(\"-\" * 50)\n",
    "for sev, count in severity_counts.items():\n",
    "    pct = (count / len(joined_data)) * 100\n",
    "    print(f\"  {sev}   | {count:>10,} | {pct:>6.2f}%\")\n",
    "\n",
    "print(\"\\nNote: Severity levels 4+, sum of all casualties in each fatal accident (1 = Possible injury, 2 = Suspected minor injury, 3 = Suspected serious injury, 4 = fatality)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize severity distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Box plot\n",
    "ax1.boxplot(joined_data['TOTAL_HARM'])\n",
    "ax1.set_title('Accidents by Severity Level', fontsize=14, fontweight='bold')\n",
    "ax1.semilogy()\n",
    "ax1.set_xlabel('Severity', fontsize=12)\n",
    "ax1.set_ylabel('Number of Accidents', fontsize=12)\n",
    "\n",
    "# Violin plot\n",
    "ax2.violinplot(joined_data['TOTAL_HARM'].astype(float), showmeans=False, showmedians=True)\n",
    "ax2.set_title('Severity Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.semilogy()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6477631",
   "metadata": {},
   "source": [
    "## 7. Weather Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb39e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top weather conditions\n",
    "weather_counts = joined_data['WEATHERNAME'].value_counts().head(15)\n",
    "\n",
    "print(\"Top 15 Weather Conditions:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (weather, count) in enumerate(weather_counts.items(), 1):\n",
    "    pct = (count / len(joined_data)) * 100\n",
    "    print(f\"{i:2d}. {weather:35s} {count:>8,}  ({pct:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d192ebd",
   "metadata": {},
   "source": [
    "## 8. Infrastructure Features\n",
    "Probably going to drop this, it's kind of difficult to do manually and we can probably find more useful information with some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eda469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Infrastructure boolean columns\n",
    "# infrastructure_cols = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', \n",
    "#                        'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', \n",
    "#                        'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']\n",
    "\n",
    "# # Handle missing values first by filling with False (assuming missing means feature not present)\n",
    "# df_clean = df[infrastructure_cols].fillna(False)\n",
    "\n",
    "# infra_data = []\n",
    "# for col in infrastructure_cols:\n",
    "#     true_count = df_clean[col].sum()\n",
    "#     false_count = len(df_clean) - true_count\n",
    "#     missing_count = df[col].isnull().sum()\n",
    "#     total_count = len(df)\n",
    "    \n",
    "#     true_percentage = (true_count / total_count) * 100\n",
    "#     false_percentage = (false_count / total_count) * 100\n",
    "#     missing_percentage = (missing_count / total_count) * 100\n",
    "    \n",
    "#     infra_data.append({\n",
    "#         'Feature': col,\n",
    "#         'True_Count': true_count,\n",
    "#         'True_Percentage': true_percentage,\n",
    "#         'False_Count': false_count,\n",
    "#         'False_Percentage': false_percentage,\n",
    "#         'Missing_Percentage': missing_percentage\n",
    "#     })\n",
    "\n",
    "# infra_stats = pd.DataFrame(infra_data).sort_values('True_Percentage', ascending=False)\n",
    "\n",
    "# print(\"Infrastructure Feature Presence:\")\n",
    "# print(\"=\" * 60)\n",
    "# print(infra_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e23896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize infrastructure features\n",
    "# fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# ax.barh(infra_stats['Feature'], infra_stats['True_Percentage'], color='steelblue')\n",
    "# ax.set_xlabel('Percentage of Accidents (%)', fontsize=12)\n",
    "# ax.set_title('Presence of Infrastructure Features in Accidents', fontsize=14, fontweight='bold')\n",
    "# ax.invert_yaxis()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be3b92",
   "metadata": {},
   "source": [
    "## 9. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS - FARS DATASET (2023)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä DATASET SIZE:\")\n",
    "print(f\"   ‚Ä¢ Total records: {len(joined_data)}\")\n",
    "print(f\"   ‚Ä¢ Features: {len(joined_data.columns)}\")\n",
    "print(f\"   ‚Ä¢ Memory usage: {joined_data.memory_usage(deep=True).sum() / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nüìç GEOGRAPHIC COVERAGE:\")\n",
    "print(f\"   ‚Ä¢ States: {joined_data['STATENAME'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Top state: {joined_data['STATENAME'].value_counts().index[0]} ({joined_data['STATENAME'].value_counts().iloc[0]:,} accidents)\")\n",
    "\n",
    "# print(\"\\n‚ö†Ô∏è SEVERITY DISTRIBUTION:\")\n",
    "# for sev in sorted(joined_data['TOTAL_HARM'].unique()):\n",
    "#     count = (joined_data['TOTAL_HARM'] == sev).sum()\n",
    "#     pct = (count / len(joined_data)) * 100\n",
    "#     print(f\"   ‚Ä¢ Level {sev}: {count:>10,} ({pct:>5.2f}%)\")\n",
    "\n",
    "print(\"\\nüå§Ô∏è WEATHER CONDITIONS:\")\n",
    "print(f\"   ‚Ä¢ Most common: {joined_data['WEATHERNAME'].value_counts().index[0]}\")\n",
    "\n",
    "# print(\"\\nüö¶ INFRASTRUCTURE:\")\n",
    "# print(f\"   ‚Ä¢ Traffic signals: {(df['Traffic_Signal'].sum() / len(df) * 100):.1f}%\")\n",
    "# print(f\"   ‚Ä¢ Crossings: {(df['Crossing'].sum() / len(df) * 100):.1f}%\")\n",
    "# print(f\"   ‚Ä¢ Junctions: {(df['Junction'].sum() / len(df) * 100):.1f}%\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è DATA QUALITY:\")\n",
    "print(f\"   ‚Ä¢ Columns with missing data: {len(missing)} / {len(joined_data.columns)}\")\n",
    "print(f\"   ‚Ä¢ Overall completeness: {(1 - joined_data.isnull().sum().sum() / (len(joined_data) * len(joined_data.columns))) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331dfeb",
   "metadata": {},
   "source": [
    "## 12. Save preprocessed Dataset with Modified Data Types and New Features\n",
    "\n",
    "Lets save the dataset with modified data types and new features for quick testing in subsequent notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_full_path = Config.FARS_CLEANED_DIR / \"fars.csv\"\n",
    "joined_data.to_csv(output_full_path, index=False)\n",
    "\n",
    "print(\"\\nDataset saved with the following details:\")\n",
    "print(f\"‚úì Cleaned dataset saved to: {output_full_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "US_accidents_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
